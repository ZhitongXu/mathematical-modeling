{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"pacifier_cleaned.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}),\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "my_stopwords = text.ENGLISH_STOP_WORDS.union(['baby','pacifier','pacifiers','product','did','does','do','just',\n",
    "                                              'buy','bought'])\n",
    "text.CountVectorizer(stop_words= my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = {1:0, 2:0, 3:2, 4:1, 5:1}\n",
    "dat['emotion'] = dat['star_rating'].map(y_dict)\n",
    "dat = dat.fillna(\"blank\")\n",
    "\n",
    "pos = dat[dat['emotion'] == 1]\n",
    "pos = pos.reset_index(drop=True)\n",
    "\n",
    "neg = dat[dat['emotion'] == 0]\n",
    "neg = neg.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos['review_body']\n",
    "neg = neg['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos[pos.isnull().values==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to record the attributes and words\n",
    "attr = {}\n",
    "# the attributes we need\n",
    "select_attr = ['JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNS', 'VB', 'VBZ', 'VBD', 'VBN', 'RBR']\n",
    "\n",
    "# Word Segmentation and POS Tagging -- select words\n",
    "for i in range(len(pos)):\n",
    "    # cut sentences\n",
    "    sentence = change_form(word_tokenize(pos[i]))\n",
    "    # tag the attributes; select words by some conditions\n",
    "    words_attr = [(x[0],x[1]) for x in pos_tag(sentence) if x[1] in select_attr and x[0].lower() not in my_stopwords and check_English(x[0].lower())]\n",
    "    # add words; group by attributes\n",
    "    for x in words_attr:\n",
    "        if x[1] in attr.keys():\n",
    "            if x[0] not in attr[x[1]]:\n",
    "                attr[x[1]].append(x[0].lower())\n",
    "        else:\n",
    "            attr[x[1]] = [x[0].lower()]\n",
    "\n",
    "# extract the words\n",
    "words = list(attr.values())\n",
    "# combine words in one list\n",
    "words_whole = []\n",
    "for i in range(len(words)):\n",
    "    words_whole.extend(words[i])\n",
    "\n",
    "# select the words in words_whole list\n",
    "def handle(x):\n",
    "    temp = [y for y in x if y in words_whole]\n",
    "    return temp\n",
    "\n",
    "# cut each sentence in review_body; put the result(words) in a list \n",
    "cut_sentences = [handle(word_tokenize(x)) for x in pos]\n",
    "\n",
    "res = []\n",
    "for li in cut_sentences:\n",
    "    s = \" \".join(li)\n",
    "    res.append(s)\n",
    "\n",
    "pos_content = pd.Series(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to record the attributes and words\n",
    "attr = {}\n",
    "# the attributes we need\n",
    "select_attr = ['JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNS', 'VB', 'VBZ', 'VBD', 'VBN', 'RBR']\n",
    "\n",
    "# Word Segmentation and POS Tagging -- select words\n",
    "for i in range(len(neg)):\n",
    "    # cut sentences\n",
    "    sentence = change_form(word_tokenize(neg[i]))\n",
    "    # tag the attributes; select words by some conditions\n",
    "    words_attr = [(x[0],x[1]) for x in pos_tag(sentence) if x[1] in select_attr and x[0].lower() not in my_stopwords and check_English(x[0].lower())]\n",
    "    # add words; group by attributes\n",
    "    for x in words_attr:\n",
    "        if x[1] in attr.keys():\n",
    "            if x[0] not in attr[x[1]]:\n",
    "                attr[x[1]].append(x[0].lower())\n",
    "        else:\n",
    "            attr[x[1]] = [x[0].lower()]\n",
    "\n",
    "# extract the words\n",
    "words = list(attr.values())\n",
    "# combine words in one list\n",
    "words_whole = []\n",
    "for i in range(len(words)):\n",
    "    words_whole.extend(words[i])\n",
    "\n",
    "# select the words in words_whole list\n",
    "def handle(x):\n",
    "    temp = [y for y in x if y in words_whole]\n",
    "    return temp\n",
    "\n",
    "# cut each sentence in review_body; put the result(words) in a list \n",
    "cut_sentences = [handle(word_tokenize(x)) for x in neg]\n",
    "\n",
    "res = []\n",
    "for li in cut_sentences:\n",
    "    s = \" \".join(li)\n",
    "    res.append(s)\n",
    "\n",
    "neg_content = pd.Series(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.030*\"loves\" + 0.029*\"son\" + 0.019*\"mouth\" + 0.015*\"little\" + 0.014*\"use\" + 0.014*\"animal\" + 0.013*\"hold\" + 0.013*\"daughter\" + 0.012*\"ones\" + 0.010*\"great\"'), (1, '0.033*\"loves\" + 0.028*\"cute\" + 0.025*\"daughter\" + 0.022*\"mouth\" + 0.022*\"great\" + 0.020*\"paci\" + 0.017*\"son\" + 0.016*\"little\" + 0.014*\"hold\" + 0.014*\"old\"'), (2, '0.018*\"price\" + 0.018*\"great\" + 0.014*\"perfect\" + 0.013*\"love\" + 0.013*\"like\" + 0.012*\"hospital\" + 0.012*\"shipping\" + 0.012*\"ones\" + 0.010*\"fast\" + 0.008*\"rubber\"'), (3, '0.014*\"paci\" + 0.013*\"got\" + 0.013*\"little\" + 0.012*\"really\" + 0.010*\"use\" + 0.009*\"easy\" + 0.008*\"gift\" + 0.008*\"big\" + 0.008*\"loved\" + 0.008*\"shower\"'), (4, '0.018*\"binky\" + 0.017*\"months\" + 0.016*\"use\" + 0.014*\"son\" + 0.013*\"like\" + 0.012*\"child\" + 0.012*\"mouth\" + 0.011*\"soothie\" + 0.010*\"time\" + 0.010*\"daughter\"'), (5, '0.043*\"love\" + 0.036*\"\" + 0.017*\"son\" + 0.016*\"like\" + 0.015*\"mouth\" + 0.014*\"loved\" + 0.012*\"face\" + 0.010*\"shape\" + 0.009*\"avent\" + 0.009*\"loves\"'), (6, '0.037*\"like\" + 0.017*\"nipple\" + 0.016*\"good\" + 0.014*\"daughter\" + 0.012*\"mouth\" + 0.011*\"old\" + 0.009*\"little\" + 0.009*\"great\" + 0.009*\"likes\" + 0.009*\"months\"'), (7, '0.026*\"gift\" + 0.026*\"love\" + 0.019*\"little\" + 0.016*\"great\" + 0.015*\"colors\" + 0.014*\"bag\" + 0.013*\"shower\" + 0.013*\"use\" + 0.013*\"nice\" + 0.010*\"diaper\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora,models\n",
    "\n",
    "data_positive = pos_content.apply(lambda x:x.split(' '))\n",
    "\n",
    "pos_dict = corpora.Dictionary(data_positive)\n",
    "pos_corpus = [pos_dict.doc2bow(i) for i in data_positive]\n",
    "pos_lda = models.LdaModel(corpus=pos_corpus,num_topics=8,id2word=pos_dict)\n",
    "\n",
    "print(pos_lda.print_topics(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.025*\"like\" + 0.011*\"paci\" + 0.010*\"really\" + 0.010*\"son\" + 0.009*\"got\" + 0.008*\"color\" + 0.008*\"hard\" + 0.008*\"old\" + 0.008*\"plastic\" + 0.007*\"good\"'), (1, '0.012*\"mouth\" + 0.011*\"use\" + 0.009*\"like\" + 0.009*\"nipple\" + 0.008*\"little\" + 0.008*\"babies\" + 0.007*\"daughter\" + 0.006*\"better\" + 0.006*\"big\" + 0.006*\"son\"'), (2, '0.011*\"like\" + 0.011*\"little\" + 0.010*\"pink\" + 0.010*\"use\" + 0.009*\"nipple\" + 0.009*\"really\" + 0.008*\"mouth\" + 0.006*\"time\" + 0.006*\"wash\" + 0.006*\"ones\"'), (3, '0.011*\"son\" + 0.010*\"month\" + 0.010*\"big\" + 0.010*\"different\" + 0.009*\"used\" + 0.009*\"cute\" + 0.009*\"old\" + 0.008*\"mouth\" + 0.008*\"right\" + 0.007*\"babies\"'), (4, '0.016*\"use\" + 0.010*\"mouth\" + 0.010*\"bag\" + 0.010*\"son\" + 0.009*\"got\" + 0.009*\"like\" + 0.008*\"great\" + 0.006*\"really\" + 0.006*\"colors\" + 0.006*\"diaper\"'), (5, '0.014*\"months\" + 0.013*\"son\" + 0.012*\"use\" + 0.011*\"month\" + 0.011*\"old\" + 0.011*\"mouth\" + 0.010*\"time\" + 0.010*\"child\" + 0.009*\"good\" + 0.008*\"got\"'), (6, '0.023*\"like\" + 0.019*\"mouth\" + 0.012*\"ones\" + 0.010*\"animal\" + 0.010*\"nipple\" + 0.010*\"son\" + 0.009*\"daughter\" + 0.007*\"received\" + 0.007*\"way\" + 0.007*\"paci\"'), (7, '0.037*\"like\" + 0.019*\"mouth\" + 0.012*\"son\" + 0.011*\"really\" + 0.011*\"nipple\" + 0.009*\"hard\" + 0.008*\"daughter\" + 0.008*\"use\" + 0.008*\"little\" + 0.007*\"babies\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora,models\n",
    "\n",
    "data_negative = neg_content.apply(lambda x:x.split(' '))\n",
    "\n",
    "neg_dict = corpora.Dictionary(data_negative)\n",
    "neg_corpus = [neg_dict.doc2bow(i) for i in data_negative]\n",
    "#放进gensim.models中的lda模型进行训练\n",
    "neg_lda = models.LdaModel(corpus=neg_corpus,num_topics=8,id2word=neg_dict)\n",
    "\n",
    "print(neg_lda.print_topics(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
